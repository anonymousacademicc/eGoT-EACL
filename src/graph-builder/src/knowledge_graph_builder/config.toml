[llm]
model = "llama-4-scout"
#model = "claude-3.5-sonnet-v2"
#model = "gpt4o"
#model = "llama3-2-90b-instruct-v1:0"
api_key = "sk-1234"
base_url = "https://llm.jetstream-cloud.org/llama-4-scout/v1/completions"
#base_url = "http://localhost:4000/v1/chat/completions"
max_tokens = 8192
#max_tokens = 4096
temperature = 0.8

[embedding]
# Model used for generating text embeddings. text-embedding-3-small is a good default.
model = "all-MiniLM-L6-v2"

[neo4j]
# Connection details for your Neo4j AuraDB or local instance
uri = "bolt://ip_address:7687"
user = "neo4j"
password = "neo4j@ndl"
enabled = true
batch_size = 10  # Number of nodes to process in each batch

[chunking]
chunk_size = 100  # Number of words per chunk
overlap = 20     # Number of words to overlap between chunks

[standardization]
enabled = true             # Whether to enable entity standardization
use_llm_for_entities = true  # Whether to use LLM for additional entity resolution

[inference]
enabled = true             # Whether to enable relationship inference
use_llm_for_inference = true  # Whether to use LLM for relationship inference
apply_transitive = true    # Whether to apply transitive inference rules

[visualization]
edge_smooth = "dynamic"  # Options: false, "dynamic", "continuous", "discrete", "diagonalCross", 
                         # "straightCross", "horizontal", "vertical", "curvedCW", "curvedCCW", "cubicBezier": true = "continuous"
